{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZpMrdru3LEM"
      },
      "outputs": [],
      "source": [
        "# Updating and installing strace to the Linux Terminal\n",
        "!sudo apt update && sudo apt install strace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess # Library linking work in python with linux terminal\n",
        "import re\n",
        "import csv\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "DcoprA7wcbJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables to maintain state\n",
        "lof_model = None # Local Outlier Factor Model\n",
        "feature_scaler = None\n",
        "is_trained = False"
      ],
      "metadata": {
        "id": "_hesUvQAdPLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the dataset of benign binaries\n",
        "!git clone https://github.com/packing-box/dataset-packed-elf"
      ],
      "metadata": {
        "id": "DxBVuYNWdMN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: 'test_binaries' folder has a set of malicious binaries already uploaded.\n",
        "# Seperating into training and test binaries set.\n",
        "# not-packed (training set) and test_binaries (test set); of binaries.\n",
        "!mv ./dataset-packed-elf/not-packed/{cat, cp, cut, date, dir, echo, head, less, ls, mv} ./test_binaries/"
      ],
      "metadata": {
        "id": "lQKV5OzFdKwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trace_binary(binary_path, timeout=5):\n",
        "    # Trace binary execution using strace and return system calls\n",
        "    if not os.path.exists(binary_path):\n",
        "        print(f\"Error: Binary not found: {binary_path}\")\n",
        "        return []\n",
        "\n",
        "    if not os.access(binary_path, os.X_OK):\n",
        "        print(f\"Error: Binary not executable: {binary_path}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # Use -yy for better socket and file descriptor info\n",
        "        # Use -f to follow 'child' processes\n",
        "        cmd = ['strace', '-yy', '-f', '-e', 'trace=all', binary_path]\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=timeout,\n",
        "            stdin=subprocess.DEVNULL  # Prevent waiting for input (Timeout)\n",
        "        )\n",
        "        return parse_strace_output(result.stderr)\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        print(f\"Timeout tracing {binary_path}\")\n",
        "        # Try to capture any partial output\n",
        "        if e.stderr:\n",
        "            return parse_strace_output(e.stderr.decode('utf-8', errors='ignore'))\n",
        "        return []\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: strace not installed or binary not found\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error tracing {binary_path}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "bZlWkpiwdHuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_strace_output(strace_output):\n",
        "    # Parse strace output to extract system call names\n",
        "    system_calls = []\n",
        "    # More robust pattern that handles process IDs and other prefixes\n",
        "    call_pattern = r'^(?:\\d+\\s+)?(\\w+)\\('\n",
        "\n",
        "    for line in strace_output.split('\\n'):\n",
        "        line = line.strip()\n",
        "        # Skip various strace metadata lines\n",
        "        if (not line or\n",
        "            line.startswith('+++') or\n",
        "            line.startswith('---') or\n",
        "            'unfinished' in line or\n",
        "            'resumed' in line):\n",
        "            continue\n",
        "\n",
        "        match = re.match(call_pattern, line)\n",
        "        if match:\n",
        "            system_call = match.group(1)\n",
        "            # Additional filtering\n",
        "            if (system_call and\n",
        "                not system_call.isdigit() and\n",
        "                not system_call.startswith('+++') and\n",
        "                not system_call.startswith('---')):\n",
        "                system_calls.append(system_call)\n",
        "\n",
        "    return system_calls"
      ],
      "metadata": {
        "id": "j9uddTuXdEKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_10_features(system_calls):\n",
        "    # Extract exactly 10 pure features from strace logs\n",
        "    if not system_calls:\n",
        "        return np.zeros(10)\n",
        "\n",
        "    call_counter = Counter(system_calls)\n",
        "    total_calls = len(system_calls)\n",
        "    unique_calls = len(call_counter)\n",
        "\n",
        "    # Feature definitions\n",
        "    total_call_count = total_calls\n",
        "\n",
        "    unique_call_ratio = unique_calls / total_calls if total_calls > 0 else 0\n",
        "\n",
        "    file_ops = ['open', 'openat', 'read', 'write', 'close', 'stat', 'fstat', 'lseek']\n",
        "    file_op_count = sum(call_counter.get(op, 0) for op in file_ops)\n",
        "    file_op_ratio = file_op_count / total_calls if total_calls > 0 else 0\n",
        "\n",
        "    process_ops = ['fork', 'clone', 'execve', 'wait4', 'waitpid']\n",
        "    process_op_count = sum(call_counter.get(op, 0) for op in process_ops)\n",
        "    process_op_ratio = process_op_count / total_calls if total_calls > 0 else 0\n",
        "\n",
        "    network_ops = ['socket', 'connect', 'bind', 'accept', 'sendto', 'recvfrom']\n",
        "    network_op_count = sum(call_counter.get(op, 0) for op in network_ops)\n",
        "    network_op_ratio = network_op_count / total_calls if total_calls > 0 else 0\n",
        "\n",
        "    memory_ops = ['mmap', 'mprotect', 'brk', 'munmap']\n",
        "    memory_op_count = sum(call_counter.get(op, 0) for op in memory_ops)\n",
        "    memory_op_ratio = memory_op_count / total_calls if total_calls > 0 else 0\n",
        "\n",
        "    security_ops = ['ptrace', 'chmod', 'chown', 'setuid', 'setgid', 'capset']\n",
        "    security_op_count = sum(call_counter.get(op, 0) for op in security_ops)\n",
        "    security_op_ratio = security_op_count / total_calls if total_calls > 0 else 0\n",
        "\n",
        "    most_frequent_ratio = max(call_counter.values()) / total_calls if call_counter else 0\n",
        "\n",
        "    # Entropy calculation\n",
        "    entropy = 0\n",
        "    for count in call_counter.values():\n",
        "        p = count / total_calls\n",
        "        entropy -= p * np.log2(p) if p > 0 else 0\n",
        "\n",
        "    # Transition rate calculation\n",
        "    transitions = 0\n",
        "    for i in range(1, len(system_calls)):\n",
        "        if system_calls[i] != system_calls[i-1]:\n",
        "            transitions += 1\n",
        "    transition_rate = transitions / total_calls if total_calls > 0 else 0\n",
        "\n",
        "    features = [\n",
        "        total_call_count,      # Feature 1\n",
        "        unique_call_ratio,     # Feature 2\n",
        "        file_op_ratio,         # Feature 3\n",
        "        process_op_ratio,      # Feature 4\n",
        "        network_op_ratio,      # Feature 5\n",
        "        memory_op_ratio,       # Feature 6\n",
        "        security_op_ratio,     # Feature 7\n",
        "        most_frequent_ratio,   # Feature 8\n",
        "        entropy,               # Feature 9\n",
        "        transition_rate        # Feature 10\n",
        "    ]\n",
        "\n",
        "    return np.array(features)"
      ],
      "metadata": {
        "id": "UXw5qqWjc987"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_binary_dataset(binaries_dir, output_csv=\"binary_dataset.csv\", timeout=5):\n",
        "    # Create CSV dataset from all binaries in directory\n",
        "\n",
        "    if not os.path.exists(binaries_dir):\n",
        "        print(f\"Error: Directory {binaries_dir} does not exist\")\n",
        "        return\n",
        "\n",
        "    binaries = []\n",
        "    for file in os.listdir(binaries_dir):\n",
        "        file_path = os.path.join(binaries_dir, file)\n",
        "        if os.path.isfile(file_path) and os.access(file_path, os.X_OK):\n",
        "            binaries.append(file_path)\n",
        "\n",
        "    print(f\"Found {len(binaries)} executables in {binaries_dir}\")\n",
        "\n",
        "    headers = ['binary_path', 'total_call_count', 'unique_call_ratio', 'file_op_ratio',\n",
        "               'process_op_ratio', 'network_op_ratio', 'memory_op_ratio', 'security_op_ratio',\n",
        "               'most_frequent_ratio', 'entropy', 'transition_rate', 'trace_successful']\n",
        "\n",
        "    with open(output_csv, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for binary_path in binaries:\n",
        "            print(f\"Processing {os.path.basename(binary_path)}...\")\n",
        "            try:\n",
        "                system_calls = trace_binary(binary_path, timeout)\n",
        "                features = extract_10_features(system_calls)\n",
        "                row = [binary_path] + features.tolist() + [len(system_calls) > 0]\n",
        "                writer.writerow(row)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed {binary_path}: {e}\")\n",
        "                writer.writerow([binary_path] + [0]*10 + [False])\n",
        "\n",
        "    print(f\"Dataset saved to {output_csv}\")\n",
        "\n",
        "create_binary_dataset(\"dataset-packed-elf/not-packed/\")"
      ],
      "metadata": {
        "id": "SFa3T7OZc81v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lof_detector(csv_file, n_neighbors=15, contamination=0.1, model_file=\"lof_model.pkl\"):\n",
        "    # Train the LOF model from CSV dataset and save to file\n",
        "    global lof_model, feature_scaler, is_trained\n",
        "\n",
        "    print(f\"Training LOF detector from {csv_file}...\")\n",
        "\n",
        "    # Read CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Filter successful traces only\n",
        "    successful_df = df[df['trace_successful'] == True]\n",
        "\n",
        "    if len(successful_df) < 5:\n",
        "        raise ValueError(f\"Need at least 5 training samples, got {len(successful_df)}\")\n",
        "\n",
        "    # Extract feature columns (exclude binary_path and trace_successful)\n",
        "    feature_columns = [col for col in successful_df.columns if col not in ['binary_path', 'trace_successful']]\n",
        "    X_train = successful_df[feature_columns].values\n",
        "\n",
        "    # Scale features\n",
        "    feature_scaler = StandardScaler()\n",
        "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
        "\n",
        "    # Train LOF model\n",
        "    lof_model = LocalOutlierFactor(\n",
        "        n_neighbors=min(n_neighbors, len(X_train_scaled) - 1),\n",
        "        contamination=contamination,\n",
        "        novelty=True\n",
        "    )\n",
        "    lof_model.fit(X_train_scaled)\n",
        "\n",
        "    # Save model and scaler to file\n",
        "    model_data = {\n",
        "        'lof_model': lof_model,\n",
        "        'feature_scaler': feature_scaler,\n",
        "        'feature_columns': feature_columns\n",
        "    }\n",
        "\n",
        "    with open(model_file, 'wb') as f:\n",
        "        pickle.dump(model_data, f)\n",
        "\n",
        "    is_trained = True\n",
        "    print(f\"Training completed with {len(successful_df)} samples\")\n",
        "    print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "    print(f\"Model saved to {model_file}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "train_lof_detector(\"binary_dataset.csv\")"
      ],
      "metadata": {
        "id": "YhDJY9fLc03V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomaly(model_path, test_binaries_folder):\n",
        "    # Detect anomalies in all binaries in test folder using saved model\n",
        "    # Load model\n",
        "    with open(model_path, 'rb') as f:\n",
        "        model_data = pickle.load(f)\n",
        "\n",
        "    lof_model = model_data['lof_model']\n",
        "    feature_scaler = model_data['feature_scaler']\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Process all binaries in test folder\n",
        "    for file in os.listdir(test_binaries_folder):\n",
        "        binary_path = os.path.join(test_binaries_folder, file)\n",
        "        if os.path.isfile(binary_path) and os.access(binary_path, os.X_OK):\n",
        "            try:\n",
        "                system_calls = trace_binary(binary_path)\n",
        "                if not system_calls or len(system_calls) < 3:\n",
        "                    results.append((binary_path, True, -1.0))  # Too few calls = anomalous\n",
        "                    continue\n",
        "\n",
        "                features = extract_10_features(system_calls)\n",
        "                features_scaled = feature_scaler.transform([features])\n",
        "\n",
        "                prediction = lof_model.predict(features_scaled)[0]\n",
        "                anomaly_score = lof_model.decision_function(features_scaled)[0]\n",
        "\n",
        "                is_anomalous = prediction == -1\n",
        "                results.append((binary_path, is_anomalous, anomaly_score))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {binary_path}: {e}\")\n",
        "                results.append((binary_path, True, -1.0))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "3Zl809vncl1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_detection_report(test_binaries_folder):\n",
        "    # Print detection results as table with accuracy and F1 score\n",
        "    import os\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "    results = detect_anomaly(\"./lof_model.pkl\", test_binaries_folder)\n",
        "\n",
        "    # Extract predictions and ground truth\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    print(\"DETECTION RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Binary':<20} {'Verdict':<8} {'Score':<12} {'Status':<6}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for binary_path, is_anomalous, score in results:\n",
        "        binary_name = os.path.basename(binary_path)\n",
        "\n",
        "        # Truncate long filenames\n",
        "        if len(binary_name) > 18:\n",
        "            if '.elf' in binary_name:\n",
        "                # For .elf files, show first 8 chars + ... + last 6 chars\n",
        "                binary_name = binary_name[:8] + '...' + binary_name[-6:]\n",
        "            else:\n",
        "                binary_name = binary_name[:15] + '...'\n",
        "\n",
        "        # Determine ground truth: .elf files are malware, others are legit\n",
        "        is_malware_truth = '.elf' in binary_name or binary_name.endswith('.elf')\n",
        "        y_true.append(1 if is_malware_truth else 0)\n",
        "        y_pred.append(1 if is_anomalous else 0)\n",
        "\n",
        "        verdict = \"MALWARE\" if is_anomalous else \"LEGIT\"\n",
        "        status = \"TRUE\" if is_anomalous == is_malware_truth else \"FALSE\"\n",
        "\n",
        "        print(f\"{binary_name:<20} {verdict:<8} {score:<12.3f} {status:<6}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    # Detailed breakdown\n",
        "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
        "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n",
        "    tn = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 0)\n",
        "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n",
        "\n",
        "    print(f\"\\nPERFORMANCE METRICS\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    print(f\"Precision: {tp/(tp+fp) if (tp+fp) > 0 else 0:.4f}\")\n",
        "    print(f\"Recall:    {tp/(tp+fn) if (tp+fn) > 0 else 0:.4f}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"TP: {tp}  FP: {fp}\")\n",
        "    print(f\"FN: {fn}  TN: {tn}\")\n",
        "# TP: True Positive; FP: False Positive; FN: False Negative; TN: True Negative"
      ],
      "metadata": {
        "id": "7hJgVxQOckrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}